{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Python for Scientific Data Analysis_\n",
    "\n",
    "# NumPy/SciPy\n",
    "\n",
    "## Section 5: Basic Linear Algebra with NumPy and SciPy\n",
    "\n",
    "Linear Algebra is a cornerstone of computational mathematics.   The simple linear algebra you probably learned in high school or undergrad consists of small vectors and matrices.   But the real power with linear algebra rests in very large matrices and vectors.  Since you cannot realistically solve these by hand, computers come in.  Hence, the importance of numerical linear algebra.\n",
    "\n",
    "Both NumPy and SciPy have numerous linear algebra routines.  In this section, we will focus on the simplest ones with NumPy, describing how to do basic linear algebra operations.    \n",
    "\n",
    "Later in the course, after we have learned _Matplotlib_, _Pandas_, and _AstroPy_ , we will build upon this foundation to describe more advanced linear algebra operations with both NumPy and SciPy and advanced modeling with SciPy.   But for now, this short introduction to linear algebra and modeling will allow us to do useful things with packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Vectors and Matrices with NumPy\n",
    "\n",
    "Creating vectors and matrices for linear algebra operations is straightforward with NumPy.   In fact, you have done them already.  E.g. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a_vector=np.array([1,2,3,4,5,6]) #orientless\n",
    "#array([1, 2, 3, 4, 5, 6])\n",
    "a_vector.shape\n",
    "#(6,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_row_vector=np.array([[1,2,3,4,5,6]])\n",
    "#array([[1, 2, 3, 4, 5, 6]])\n",
    "a_row_vector.shape\n",
    "#(1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_column_vector=np.array([[1],[2],[3],[4],[5],[6]])\n",
    "\n",
    "\n",
    "a_column_vector.shape\n",
    "#(6, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_matrix=np.array([[1,2,3],[4,5,6]])\n",
    "a_matrix.shape\n",
    "#(2,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first example -- _a\\_vector_ -- is orientationless: it is neither a row nor column vector but just a 1D list of numbers in NumPy.  Orientation in NumPy is given by brackets ``[ ]``.  The outermost brackets group all of the numbers together into one object.   Then, each additional set of brackets indicates a row: so a row vector has all numbers in one row, while a column vector has multiple rows, with each row containing one number.   A matrix then will have multiple rows with each row containing more than one number.\n",
    "\n",
    "Array arithmetic rules -- e.g. adding, subtracting -- follow those described in Section 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Linear Algebra Operations\n",
    "\n",
    "#### _Vector Norms_\n",
    "\n",
    "The magnitude or _norm_ of a vector is the distance from head to tail of the vector.  It is computed using the standard Euclidean distance formula: the square-root of the sum of squared vector elements:\n",
    "\n",
    "||v||$_{2}$ = $\\sqrt{\\sum_{i=1}^{n} v_{i}^{2}}$\n",
    "\n",
    "In Python this can be computed as \n",
    "``np.linalg.norm(v)``. \n",
    "\n",
    " Note, since technically this is the $L_{\\rm 2}$ norm the `v` has that subscript of `2`.  There are other norms -- the $L_{\\rm 1}$ (\"Manhattan\" norm) which is ||v||$_{1}$ = $\\sum_{i=1}^{n} |v_{i}|$ (i.e. the sum of the absolute value of array elements), etc. -- but you can read up on those if you want in case you don't already know them.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of computing the vector norm with NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a=np.array([[3,4,5,6,7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.61895003862225"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(a)\n",
    "#11.61895003862225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.61895003862225"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(a,2) #note: same thing ... you are asking NumPy to compute the L2 norm\n",
    "#11.61895003862225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(a,1) #note: different! ... you are asking NumPy to compute the L1 or 'Manhattan' norm\n",
    "#7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the full capability of ``norm`` is complex and best left for official documentation, which is here:[https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html]()\n",
    "\n",
    "\n",
    "Sometimes we don't want the vector as-is but want a corresponding _unit vector_, where ||v|| = 1.   To produce this unit vector you have to take the input vector and divide by its norm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Matrix Determinant_\n",
    "\n",
    "The _determinant_ is a key property associated with a square matrix.   It has a special importance when we later get to inverting matrices or do eigendecomposition of them.   The determinant of matrix $A$ is usually noted as $det(A)$ or |A|.  \n",
    "\n",
    "As you might remember, it's pretty easy to calculate determinants for small matrices, e.g. 2x2:\n",
    "\n",
    "$\\left[ {\\begin{array}{cc}\n",
    "   a & b \\\\\n",
    "   c & d \\\\\n",
    "  \\end{array} } \\right]$ \n",
    "  = $ad-bc$\n",
    "  \n",
    "For much larger matrices this gets quickly unwieldy, although you may remember how to use cofactor expansion to compute the determinants of slightly larger matrices:\n",
    "\n",
    "\n",
    "$\\left[ {\\begin{array}{cc}\n",
    "   1 & 2  & 3\\\\\n",
    "   4 & 5  & 6\\\\\n",
    "   7 & 8 & 9\n",
    "  \\end{array} } \\right]$ \n",
    "  = 1$\\left[ {\\begin{array}{cc}\n",
    "   5 & 6 \\\\\n",
    "   8 & 9 \\\\\n",
    "  \\end{array} } \\right]$ - 2$\\left[ {\\begin{array}{cc}\n",
    "   4 & 6 \\\\\n",
    "   7 & 9 \\\\\n",
    "  \\end{array} } \\right]$ + 3$\\left[ {\\begin{array}{cc}\n",
    "   4 & 5 \\\\\n",
    "   7 & 8 \\\\\n",
    "  \\end{array} } \\right]$ = -3 +12 - 9 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully, we can just ask NumPy:\n",
    "\n",
    "``matrix_det=np.linalg.det([matrix])``.\n",
    "\n",
    "Or SciPy\n",
    "\n",
    "``from scipy import linalg; matrix_det=linalg.det([matrix])``.\n",
    "\n",
    "e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from scipy import linalg\n",
    "\n",
    "a=np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "a.shape #3x3 matrix\n",
    "\n",
    "matrix_det=np.linalg.det(a)\n",
    "print(matrix_det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "matrix_det=linalg.det(a)\n",
    "print(matrix_det)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector and Matrix Multiplication\n",
    "\n",
    "The real power with NumPy's linear algebra routines come from multiplying large matrices and vectors, which we obviously cannot do by hand.   Routines for how to do this are below ...\n",
    "\n",
    "#### _Dot Product_\n",
    "\n",
    "The ``dot`` operation does vector-vector, matrix-vector, and matrix-matrix multiplication of two arrays: ``np.dot([first array],[second array])``. E.g. ``np.dot(a,b)``.\n",
    "\n",
    "Mathematically, what this operation does for each is:\n",
    "\n",
    "The vector-vector operation for each element $i$, resulting in scalar $s$ from vectors $x$ and $y$:\n",
    "\n",
    " $s = \\sum_{i} x_{i} y_{i}$\n",
    " \n",
    " The matrix-vector operation, for each $i$ row and $j$ column, resulting in vector $y$ from matrix $A$ and vector $x$:\n",
    "\n",
    " $y_{i} = \\sum_{j} A_{ij} x_{j}$\n",
    " \n",
    " \n",
    "  The vector-matrix operation, for each $i$ row and $j$ column, resulting in vector $y$ from matrix $A$ and vector $x$:\n",
    "\n",
    " $y_{j} = \\sum_{i}  x_{i} A_{ij}$\n",
    " \n",
    " \n",
    " The matrix-matrix operation, resulting in matrix $C$ from matrices $A$ and $B$:\n",
    " \n",
    " $C_{ij} = \\sum_{k} A_{ik} B_{kj}$\n",
    "\n",
    "\n",
    "Again, in all cases, the operation is ``np.dot([first array],[second array])``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note, it is quite easy to get broadcasting errors when doing matrix-vector or vector-matrix operations, so here are examples of how this works:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5],\n",
       "       [11]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    " a=np.array([[1,2],[3,4]]) #2x2 matrix 1, 2 for row =0 and 3,4 for row =1\n",
    " b=np.array([[1,2]]) #a row vector\n",
    " b2=np.array([[1],[2]]) #a column vector\n",
    " \n",
    " #np.dot(a,b) #can't do this\n",
    " np.dot(a,b2) #matrix-vector multiplication\n",
    " #([[5],[11]]) #column vector\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7, 10]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #np.dot(b2,a) #can't do this\n",
    " np.dot(b,a)  #vector-matrix multiplication\n",
    " #([[ 7, 10]]) #row vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Np.Matmul_\n",
    "\n",
    "NumPy provides another, simpler routine for _matrix_ multiplication, called (unsurprisingly) ``np.matmul([first matrix,second matrix])``.  \n",
    "\n",
    "E.g. in our previous example of matrices $A$ and $B$ this matrix multiplicaiton results in matrix $C$:\n",
    " \n",
    " $C_{ij} = \\sum_{k} A_{ik} B_{kj}$\n",
    " \n",
    " For this case, the call is ``np.matmul(A,B)``.\n",
    " \n",
    " Now, there will be plenty of other matrix operations of greater sophistication and power.  But we will get to these later.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Outer Product_\n",
    " \n",
    " Here's another useful operation: the _outer product_.   The outer product of two vectors creates a matrix.  In order to make this work, one vector must be a column vector and another must be a row vector.\n",
    " \n",
    " e.g. \n",
    "\n",
    "  $\\left[ {\\begin{array}{cc}\n",
    "   a \\\\ b \\\\\n",
    "   c \\\\ \n",
    "  \\end{array} } \\right]$   $\\left[ {\\begin{array}{cc}\n",
    "   d & e \\\\\n",
    "  \\end{array} } \\right]$ \n",
    "  = $\\left[ {\\begin{array}{cc}\n",
    "   ad & ae \\\\\n",
    "   bd & be \\\\\n",
    "   cd & ce \\\\\n",
    "  \\end{array} } \\right]$ \n",
    "  \n",
    "  In code, the outer product is computed as ``np.outer(a,b)``.   \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example:\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " v1=np.array([[1,2,3,4]])\n",
    " #array([[1, 2, 3, 4]])\n",
    " v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.5],\n",
       "       [3. ],\n",
       "       [3.5]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " v2=np.array([[5],[6],[7]])*0.5\n",
    " #array([[2.5],\n",
    " #      [3. ],\n",
    " #      [3.5]])\n",
    " v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.5,  3. ,  3.5],\n",
       "       [ 5. ,  6. ,  7. ],\n",
       "       [ 7.5,  9. , 10.5],\n",
       "       [10. , 12. , 14. ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " mat1=np.outer(v1,v2)\n",
    " #array([[ 2.5,  3. ,  3.5],\n",
    " #      [ 5. ,  6. ,  7. ],\n",
    " #      [ 7.5,  9. , 10.5],\n",
    " #      [10. , 12. , 14. ]])\n",
    " \n",
    " mat1\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note also, the ``np.dot`` function can _also_ compute an outer product PROVIDED that the first vector is a column vector and second is a row vector. \n",
    " \n",
    " e.g. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[143 351]\n",
      " [352 864]] [[143 351]\n",
      " [352 864]]\n",
      "(2, 2) (2, 2) (1, 2) (2, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    " b=np.array([[11,27]]) #a row vector\n",
    " b2=np.array([[13],[32]]) #a column vector\n",
    " \n",
    " result=np.dot(b2,b)\n",
    " \n",
    "  #array([[143, 351],\n",
    "  #     [352, 864]]) \n",
    " \n",
    " result2=np.outer(b2,b)\n",
    " \n",
    " #array([[143, 351],\n",
    " #     [352, 864]])\n",
    "\n",
    " print(result,result2)\n",
    " print(result.shape,result2.shape,b.shape,b2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving Basic Linear Equations\n",
    "\n",
    "Now, we can take the power of numerical linear algebra to the next level by using it to solve systems of linear equations.   **_Many_** coding problems I  encounter are really just dressed up versions of a simple task to solve linear equations.  \n",
    "\n",
    "For instance, consider the following set of linear equations:\n",
    "\n",
    "$x_{\\rm 1} + 4x_{\\rm 2} = 1\\\\\n",
    "3x_{\\rm 1} + 4x_{\\rm 2} = -4$\n",
    "\n",
    "Now, you could just solve this by hand fairly fast ($x_{\\rm 1} = -2.5$, $ x_{\\rm 2} = 0.875$). But why do that when NumPy and SciPy will solve it for you & do it faster?\n",
    "\n",
    "Recast the system of linear equations as a matrix equation of the form $\\textbf{Ax}$ = $\\textbf{b}$.  Here, $\\textbf{A}$ equals the coefficients in front of the variables: $\\left[ {\\begin{array}{cc}\n",
    "   1 & 4 \\\\\n",
    "   3 & 4 \\\\\n",
    "  \\end{array} } \\right]$ .   And $\\textbf{b}$ equals the right-hand side of the equation: a column vector: $\\left[ {\\begin{array}{cc}\n",
    "   1 \\\\\n",
    "  -4\n",
    "  \\end{array} } \\right]$.   The unknown $\\textbf{x}$ equals the two variables $x_{\\rm 1}$ and $x_{\\rm 2}$.   \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  In Python this is written as:\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ```\n",
    "  a=np.array([[1,4,],[3,4]]) #a two-by-two matrix\n",
    "  b=np.array([[1],[-4]]) #a column matrix\n",
    "  ```\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  We can then solve this system of linear equations by matrix _inversion_.  Thankfully, NumPy and SciPy have a LOT of tools to do this in various ways.  For now, we will focus on the most obvious, brute-force way: a simple inversion:\n",
    "  \n",
    "  $\\textbf{x}$=$\\textbf{A}^{-1} \\textbf{b}$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Here are various ways to solve it in NumPy:\n",
    "  \n",
    "  ``np.linalg.solve``\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.5  ]\n",
      " [ 0.875]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "  a=np.array([[1,4,],[3,4]]) #a two-by-two matrix\n",
    "  b=np.array([[1],[-4]]) #a column matrix\n",
    "\n",
    "  x=np.linalg.solve(a,b)\n",
    "  #array([[-2.5  ],\n",
    "  #     [ 0.875]])\n",
    "\n",
    "  print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Using ``np.linalg.inv`` (the inversion function) and ``np.dot``\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.5  ],\n",
       "       [ 0.875]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "  a=np.array([[1,4,],[3,4]]) #a two-by-two matrix\n",
    "  b=np.array([[1],[-4]]) #a column matrix\n",
    "  \n",
    "  np.linalg.inv(a).dot(b)\n",
    "  #array([[-2.5  ],\n",
    "  #     [ 0.875]])\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  Using SciPy's``linalg.solve``\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.5  ],\n",
       "       [ 0.875]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "  from scipy import linalg\n",
    "  linalg.solve(a,b)\n",
    "  #array([[-2.5  ],\n",
    "   #    [ 0.875]])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  Using SciPy's ``linalg.inv`` and ``np.dot``\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.5  ],\n",
       "       [ 0.875]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "  from scipy import linalg\n",
    "  a=np.array([[1,4,],[3,4]]) #a two-by-two matrix\n",
    "  b=np.array([[1],[-4]]) #a column matrix\n",
    "  \n",
    "  linalg.inv(a).dot(b)\n",
    "  #array([[-2.5  ],\n",
    "   #    [ 0.875]])\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "### Simple Example: Curve Fitting\n",
    "\n",
    "You can use treat curve fitting as a set of linear equations to solve and thus use matrix inversions to determine a functional fit to the data.  \n",
    "\n",
    "Here's a super-simple example\n",
    "\n",
    "![](./figures/Figure_1.png)\n",
    "\n",
    "Here we have three data points, where x = [1,2,3] and y = [10,26,50].   You might think (rightly) that this can be fit by a quadratic equation, or $y$ = $a_{o}$ + $a_{1}$x+$a_{2}$x$^{2}$.\n",
    "\n",
    "So ... what do we know?\n",
    "when x = 1, then we have 10 = $a_{o}$ + $a_{1}$ + $a_{2}$.\n",
    "when x = 2, then we hae 26 = $a_{o}$ + 2$a_{1}$ + 4$a_{2}$.\n",
    "when x = 3, then we hae 50 = $a_{o}$ + 3$a_{1}$ + 9$a_{2}$.\n",
    "\n",
    "Or ...\n",
    "\n",
    "$a_{o}$ + $a_{1}$ + $a_{2}$ = 10\n",
    "\n",
    "$a_{o}$ + 2$a_{1}$ + 4$a_{2}$ = 26\n",
    "\n",
    "$a_{o}$ + 3$a_{1}$ + 9$a_{2}$ = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I.e. a system of linear equations we can solve.  First, we can verify that we do have a unique solution:\n",
    "\n",
    "``np.linalg.det(A)`` equals ``2.0``.\n",
    "\n",
    "Now, solve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.],\n",
       "       [4.],\n",
       "       [4.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=np.array([[1,1,1],[1,2,4],[1,3,9]])\n",
    "b = np.array([10, 26, 50]).reshape((3, 1)) #convert b to a column vector\n",
    "coeff= np.linalg.inv(A).dot(b) #matrix-vector multiplication \n",
    "\n",
    "coeff\n",
    "#array([[2.],\n",
    " #      [4.],\n",
    "   #    [4.]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result tells you that $a_{0}$ = 2, $a_{1}$ = 4, and $a_{2}$ = 4 is a solution, which is confirmed by comparing the data to the model:\n",
    "\n",
    "![](./figures/Figure_2.png)\n",
    "\n",
    "(Note: in this case, it is not a *unique* solution since the determinant of A, as we find now that we have coefficients, is zero ... but small details ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Complex Example: LOCI\n",
    "\n",
    "Okay fine, the above was a simplistic example.   Does anything above help us to do actual research?  Yes! \n",
    "\n",
    "Let's pretend you are interested in imaging planets.  You have one target image $O^{T}$ where there is a planet hiding in noise (these are called \"speckles\").  You have a set of other images, $O^{R}$, with a very similar noise pattern (since \"speckles\" are _quasi-static_) from which you want to construct a _reference_ image.  These reference images could be that of a different star or the same star observed at a different parallactic angle (i.e. where north on your camera is pointed differently, so this hidden planet is in a different x,y position on your detector).\n",
    "\n",
    " After constructing this reference image, you then subtract it from the target image to reveal planets.  Sounds good?  But how do you _optimially_ combine these images?\n",
    "\n",
    "Well, if you ignore some small details (e.g. signal loss from the planet when doing all of these, etc.) this is really a question of\n",
    "\n",
    " \"how do you optimially weight each of those reference images so that the subtraction minimizes the sum of the squared residuals.\"  I.e. \n",
    " \n",
    " min($\\sigma^{2}$), where \n",
    " \n",
    " $\\sigma^{2} = \\sum_{i} (O^{T}-O^{R}_{i})^{2}$ \n",
    " \n",
    " and $i$ is the pixel number (i.e. you are summing the residuals over all of the pixels of interest.\n",
    " \n",
    " If you have a bunch of images, making up your reference set $R$ this converts to\n",
    " \n",
    " $\\sigma^{2} = \\sum_{i} (O^{T}-\\sum_{k} c^{k}O^{k}_{i})^{2}$,\n",
    " \n",
    " where $c^{k}$ is the coefficient (i.e. a number) applied to the $kth$ image in your reference set.\n",
    " \n",
    " If you have found a _real_ minimum of $\\sigma^{2}$, then the partial derivative with respect to the coefficients goes to zero for **all** coefficients.  E.g. for the $jth$ coefficient then ...\n",
    " \n",
    " $\\frac{\\partial \\sigma^{2}}{\\partial c^{j}}$\n",
    " \n",
    "  $\\partial \\sigma^{2}/\\partial c^{j}$ = $\\sum_{i} -2 O^{j}_{i}$$(O^{T}-\\sum_{k} c^{k}O^{k}_{i})$ = 0\n",
    "  \n",
    "  Reversing the summation order you get\n",
    "  \n",
    "  $\\sum_{k} c^{k}$($\\sum_{i} O^{j}_{i} O^{k}_{i}$) = $\\sum_{i} O^{j}_{i} O^{T}_{i}$\n",
    "  \n",
    "   \n",
    "  \n",
    "   Now look at the left side of the equation.  The inner summation is summing over all pixels for the $jth$ reference image and set of $kth$ reference images.  The outer summation is a summation over $k$.  The right side, is multiplying the values of two particular images together $O^{j}$ (from the reference set) and $O^{T}$ your target image.\n",
    "   \n",
    "   This is just _one_ partial derivative, minimizing one coefficient.  You can do this for every single possible coefficient. I.e. the left side of the equation becomes a single vector ($c^{k}$ over all values of $k$) multiplied by a 2-D matrix (a square matrix, actually ... since the sum over k and the entire set of reference images = same dimensions.\n",
    "   \n",
    "   I.e. we have a system of linear equations\n",
    "   \n",
    "   $A_{jk}$ = $\\sum_{i} O^{j}_{i} O^{k}_{i}$\n",
    "   \n",
    "   $x^{k}$ = $c^{k}$\n",
    "   \n",
    "   $b_{j}$ = $\\sum_{i} O^{j}_{i} O^{T}_{i}$\n",
    "   \n",
    "   **A****x**=**b**\n",
    "   \n",
    "   We can then _invert_ the matrix A, multiply it by b to get the set of coefficients that minimize the noise!\n",
    "   \n",
    "   Now, this does work really well.   \n",
    "   \n",
    "   Below shows an example with a bunch of fake planets inserted: top image is a simple median combination of reference images, bottom shows this \n",
    "   _\"Optimized Combination of Images\"_ approach.  In the real algorithm, you do this subtraction in small patches of the image at once (i.e. \"locally\").  So the name of the algorithm is then the \n",
    "   \n",
    "   _**\"Locally Optimized Combination of Images\"**_ algorithm \n",
    "   (Lafreniere et al. 2007, Astrophysical Journal, 660, 770)\n",
    "   \n",
    "   ![](./figures/Figure_3.png)\n",
    "   \n",
    "   \n",
    "   \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
